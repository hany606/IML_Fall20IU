{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab-7: ANN in Keras\n",
    "\n",
    "In this lab, you will practice simple deep learning model in Pytorch.\n",
    "\n",
    "\n",
    "## Objectives:\n",
    "1. Theoretical issues\n",
    "2. Get starting in Pytorch\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Theoretical issues\n",
    "Ordinary fully connected neural nets consists of Dense layers, activations, and output layer.\n",
    "\n",
    "1. What's the difference between deep learning and normal machine learning?\n",
    "2. How does a neural network with no hidden layers and one output neuron compare to a logistic/linear regression?\n",
    "3. How does a neural network with multiple hidden layers but with linear activation and one output neuron compared to logistic/linear regression?\n",
    "4. Can the perceptron find a non-linear decision boundary?\n",
    "5. In multi-hidden layers network, what's the need of non-linear activation function?\n",
    "6. Is random weight assignment better than assigning same weights to the units in the hidden layer.\n",
    "---"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Pytorch: Getting started\n",
    "### Feed Forward Neural Network\n",
    "an artificial neural network wherein connections between the nodes do not form a cycle.\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/en/5/54/Feed_forward_neural_net.gif)\n",
    "\n",
    "## Model Desing in Pytorch\n",
    "we have three simple parts that we need to build:\n",
    "1. Data Loading process.\n",
    "2. Model building.\n",
    "3. the training loops.\n",
    "\n",
    "### 1. Data Loading\n",
    "\n",
    "Data Loading in pytorch is very easy and broken into 3 steps:\n",
    "1. Data Source.\n",
    "2. Data Transformations.\n",
    "3. Data Loader.\n",
    "\n",
    "\n",
    "\n",
    "#### Loading data\n",
    "\n",
    "Pytorch uses data loading utility which is called `DataLoader` that supports:\n",
    "automatic batching, transformation, single- and multi-process data loading and more.."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n9920512it [00:02, 3995211.77it/s]\nExtracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n0it [00:00, ?it/s]Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n32768it [00:00, 76579.01it/s]\n0it [00:00, ?it/s]Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n1654784it [00:01, 1493430.11it/s]\n0it [00:00, ?it/s]Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\nDownloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n8192it [00:00, 23726.10it/s]Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\nProcessing...\nDone!\n\n"
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch. utils.data import DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "test_batch_size = 100\n",
    "\n",
    "data_transformations = transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.1307,), (0.3081,))\n",
    "                       ])\n",
    "\n",
    "mnist_train = datasets.MNIST('../data', train=True, download=True,\n",
    "                       transform=data_transformations)\n",
    "mnist_test = datasets.MNIST('../data', train=False,\n",
    "                            transform=data_transformations)\n",
    "\n",
    "train_loader = DataLoader(mnist_train,\n",
    "                          batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(mnist_test,\n",
    "                         batch_size=test_batch_size, shuffle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Model building\n",
    "1. Defining components: <br/>\n",
    "This step is done in the constructor, where you will define the layers that will be used accordingly in the next step.\n",
    "2. Network flow: <br/>\n",
    "This step is done in the forward function. Where you will get the input batch as an argument then you will use the defined layers in the previous step to define the flow of the network then you will return the output batch.\n",
    "\n",
    "\n",
    "Pytorch is a dynamic framework, where you can use primitive python keywords with it.\n",
    "You can use if and while statements. Also, it can accepts and returns more than one batch."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 500)\n",
    "\n",
    "        # Write 3 lines to define 3 more linear layers.\n",
    "        # 2 hidden layers with number of neurons numbers: 250 and 100\n",
    "        # 1 output layer that should output 10 neurons, one for each class.\n",
    "        self.fc2 = nn.Linear(500, 250)\n",
    "        self.fc3 = nn.Linear(250, 100)\n",
    "        self.fc4 = nn.Linear(100, 10)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # the linear layers fc1, fc2, fc3, and fc4\n",
    "        # accepts only flattened input (1D batches)\n",
    "        # while the batch x is of size (batch, 28 * 28)\n",
    "        # define one line to flatten the x to be of size (batch_sz, 28 * 28)\n",
    "        x = x.view(-1, 28*28)\n",
    "        # x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "model = Net().to(device)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training loops\n",
    "After that we should define the loops over tha batches and run the training on."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "lr = 0.01\n",
    "momentum = 0.5\n",
    "seed = 1\n",
    "log_interval = 10"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "def train( model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                       100. * batch_idx / len(train_loader), loss.item()))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def test( model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # Do the same that was done in the previous function.\n",
    "            # But without backprobagating the loss and without running the optimizers\n",
    "            # As this function is only for test.\n",
    "            # write 3 lines to transform the data to the device, get the output and compute the loss\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            loss = F.nll_loss(output, target, reduction='sum')\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": " 8 [54080/60000 (90%)]\tLoss: 0.011104\nTrain Epoch: 8 [54400/60000 (91%)]\tLoss: 0.004353\nTrain Epoch: 8 [54720/60000 (91%)]\tLoss: 0.003895\nTrain Epoch: 8 [55040/60000 (92%)]\tLoss: 0.016183\nTrain Epoch: 8 [55360/60000 (92%)]\tLoss: 0.005486\nTrain Epoch: 8 [55680/60000 (93%)]\tLoss: 0.017044\nTrain Epoch: 8 [56000/60000 (93%)]\tLoss: 0.037585\nTrain Epoch: 8 [56320/60000 (94%)]\tLoss: 0.006515\nTrain Epoch: 8 [56640/60000 (94%)]\tLoss: 0.013211\nTrain Epoch: 8 [56960/60000 (95%)]\tLoss: 0.008241\nTrain Epoch: 8 [57280/60000 (95%)]\tLoss: 0.013951\nTrain Epoch: 8 [57600/60000 (96%)]\tLoss: 0.032452\nTrain Epoch: 8 [57920/60000 (97%)]\tLoss: 0.020794\nTrain Epoch: 8 [58240/60000 (97%)]\tLoss: 0.037458\nTrain Epoch: 8 [58560/60000 (98%)]\tLoss: 0.011813\nTrain Epoch: 8 [58880/60000 (98%)]\tLoss: 0.005958\nTrain Epoch: 8 [59200/60000 (99%)]\tLoss: 0.010265\nTrain Epoch: 8 [59520/60000 (99%)]\tLoss: 0.096516\nTrain Epoch: 8 [59840/60000 (100%)]\tLoss: 0.077861\n\nTest set: Average loss: 0.0000, Accuracy: 9773/10000 (98%)\n\nTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.097102\nTrain Epoch: 9 [320/60000 (1%)]\tLoss: 0.024606\nTrain Epoch: 9 [640/60000 (1%)]\tLoss: 0.016787\nTrain Epoch: 9 [960/60000 (2%)]\tLoss: 0.012263\nTrain Epoch: 9 [1280/60000 (2%)]\tLoss: 0.038267\nTrain Epoch: 9 [1600/60000 (3%)]\tLoss: 0.028354\nTrain Epoch: 9 [1920/60000 (3%)]\tLoss: 0.025454\nTrain Epoch: 9 [2240/60000 (4%)]\tLoss: 0.115485\nTrain Epoch: 9 [2560/60000 (4%)]\tLoss: 0.074795\nTrain Epoch: 9 [2880/60000 (5%)]\tLoss: 0.003368\nTrain Epoch: 9 [3200/60000 (5%)]\tLoss: 0.005779\nTrain Epoch: 9 [3520/60000 (6%)]\tLoss: 0.019103\nTrain Epoch: 9 [3840/60000 (6%)]\tLoss: 0.011564\nTrain Epoch: 9 [4160/60000 (7%)]\tLoss: 0.062972\nTrain Epoch: 9 [4480/60000 (7%)]\tLoss: 0.116542\nTrain Epoch: 9 [4800/60000 (8%)]\tLoss: 0.009401\nTrain Epoch: 9 [5120/60000 (9%)]\tLoss: 0.059185\nTrain Epoch: 9 [5440/60000 (9%)]\tLoss: 0.002821\nTrain Epoch: 9 [5760/60000 (10%)]\tLoss: 0.006463\nTrain Epoch: 9 [6080/60000 (10%)]\tLoss: 0.014777\nTrain Epoch: 9 [6400/60000 (11%)]\tLoss: 0.003743\nTrain Epoch: 9 [6720/60000 (11%)]\tLoss: 0.058264\nTrain Epoch: 9 [7040/60000 (12%)]\tLoss: 0.007367\nTrain Epoch: 9 [7360/60000 (12%)]\tLoss: 0.008313\nTrain Epoch: 9 [7680/60000 (13%)]\tLoss: 0.005520\nTrain Epoch: 9 [8000/60000 (13%)]\tLoss: 0.002875\nTrain Epoch: 9 [8320/60000 (14%)]\tLoss: 0.006769\nTrain Epoch: 9 [8640/60000 (14%)]\tLoss: 0.003192\nTrain Epoch: 9 [8960/60000 (15%)]\tLoss: 0.004216\nTrain Epoch: 9 [9280/60000 (15%)]\tLoss: 0.004630\nTrain Epoch: 9 [9600/60000 (16%)]\tLoss: 0.022975\nTrain Epoch: 9 [9920/60000 (17%)]\tLoss: 0.012309\nTrain Epoch: 9 [10240/60000 (17%)]\tLoss: 0.024189\nTrain Epoch: 9 [10560/60000 (18%)]\tLoss: 0.005807\nTrain Epoch: 9 [10880/60000 (18%)]\tLoss: 0.007955\nTrain Epoch: 9 [11200/60000 (19%)]\tLoss: 0.002164\nTrain Epoch: 9 [11520/60000 (19%)]\tLoss: 0.026675\nTrain Epoch: 9 [11840/60000 (20%)]\tLoss: 0.007488\nTrain Epoch: 9 [12160/60000 (20%)]\tLoss: 0.006860\nTrain Epoch: 9 [12480/60000 (21%)]\tLoss: 0.005685\nTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.010136\nTrain Epoch: 9 [13120/60000 (22%)]\tLoss: 0.014384\nTrain Epoch: 9 [13440/60000 (22%)]\tLoss: 0.066868\nTrain Epoch: 9 [13760/60000 (23%)]\tLoss: 0.003307\nTrain Epoch: 9 [14080/60000 (23%)]\tLoss: 0.005146\nTrain Epoch: 9 [14400/60000 (24%)]\tLoss: 0.008392\nTrain Epoch: 9 [14720/60000 (25%)]\tLoss: 0.015516\nTrain Epoch: 9 [15040/60000 (25%)]\tLoss: 0.005751\nTrain Epoch: 9 [15360/60000 (26%)]\tLoss: 0.014389\nTrain Epoch: 9 [15680/60000 (26%)]\tLoss: 0.003264\nTrain Epoch: 9 [16000/60000 (27%)]\tLoss: 0.039119\nTrain Epoch: 9 [16320/60000 (27%)]\tLoss: 0.004630\nTrain Epoch: 9 [16640/60000 (28%)]\tLoss: 0.000972\nTrain Epoch: 9 [16960/60000 (28%)]\tLoss: 0.162702\nTrain Epoch: 9 [17280/60000 (29%)]\tLoss: 0.007292\nTrain Epoch: 9 [17600/60000 (29%)]\tLoss: 0.002333\nTrain Epoch: 9 [17920/60000 (30%)]\tLoss: 0.041770\nTrain Epoch: 9 [18240/60000 (30%)]\tLoss: 0.000687\nTrain Epoch: 9 [18560/60000 (31%)]\tLoss: 0.030052\nTrain Epoch: 9 [18880/60000 (31%)]\tLoss: 0.000442\nTrain Epoch: 9 [19200/60000 (32%)]\tLoss: 0.000696\nTrain Epoch: 9 [19520/60000 (33%)]\tLoss: 0.035809\nTrain Epoch: 9 [19840/60000 (33%)]\tLoss: 0.003427\nTrain Epoch: 9 [20160/60000 (34%)]\tLoss: 0.004095\nTrain Epoch: 9 [20480/60000 (34%)]\tLoss: 0.073360\nTrain Epoch: 9 [20800/60000 (35%)]\tLoss: 0.013628\nTrain Epoch: 9 [21120/60000 (35%)]\tLoss: 0.003453\nTrain Epoch: 9 [21440/60000 (36%)]\tLoss: 0.002937\nTrain Epoch: 9 [21760/60000 (36%)]\tLoss: 0.022905\nTrain Epoch: 9 [22080/60000 (37%)]\tLoss: 0.001748\nTrain Epoch: 9 [22400/60000 (37%)]\tLoss: 0.143538\nTrain Epoch: 9 [22720/60000 (38%)]\tLoss: 0.065163\nTrain Epoch: 9 [23040/60000 (38%)]\tLoss: 0.005354\nTrain Epoch: 9 [23360/60000 (39%)]\tLoss: 0.028536\nTrain Epoch: 9 [23680/60000 (39%)]\tLoss: 0.014192\nTrain Epoch: 9 [24000/60000 (40%)]\tLoss: 0.005481\nTrain Epoch: 9 [24320/60000 (41%)]\tLoss: 0.017316\nTrain Epoch: 9 [24640/60000 (41%)]\tLoss: 0.006009\nTrain Epoch: 9 [24960/60000 (42%)]\tLoss: 0.009003\nTrain Epoch: 9 [25280/60000 (42%)]\tLoss: 0.020775\nTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.002153\nTrain Epoch: 9 [25920/60000 (43%)]\tLoss: 0.002147\nTrain Epoch: 9 [26240/60000 (44%)]\tLoss: 0.027705\nTrain Epoch: 9 [26560/60000 (44%)]\tLoss: 0.000931\nTrain Epoch: 9 [26880/60000 (45%)]\tLoss: 0.031343\nTrain Epoch: 9 [27200/60000 (45%)]\tLoss: 0.017963\nTrain Epoch: 9 [27520/60000 (46%)]\tLoss: 0.005661\nTrain Epoch: 9 [27840/60000 (46%)]\tLoss: 0.000953\nTrain Epoch: 9 [28160/60000 (47%)]\tLoss: 0.018289\nTrain Epoch: 9 [28480/60000 (47%)]\tLoss: 0.041033\nTrain Epoch: 9 [28800/60000 (48%)]\tLoss: 0.018691\nTrain Epoch: 9 [29120/60000 (49%)]\tLoss: 0.039748\nTrain Epoch: 9 [29440/60000 (49%)]\tLoss: 0.010968\nTrain Epoch: 9 [29760/60000 (50%)]\tLoss: 0.015915\nTrain Epoch: 9 [30080/60000 (50%)]\tLoss: 0.002242\nTrain Epoch: 9 [30400/60000 (51%)]\tLoss: 0.010356\nTrain Epoch: 9 [30720/60000 (51%)]\tLoss: 0.004616\nTrain Epoch: 9 [31040/60000 (52%)]\tLoss: 0.045253\nTrain Epoch: 9 [31360/60000 (52%)]\tLoss: 0.031163\nTrain Epoch: 9 [31680/60000 (53%)]\tLoss: 0.019115\nTrain Epoch: 9 [32000/60000 (53%)]\tLoss: 0.000594\nTrain Epoch: 9 [32320/60000 (54%)]\tLoss: 0.001473\nTrain Epoch: 9 [32640/60000 (54%)]\tLoss: 0.006854\nTrain Epoch: 9 [32960/60000 (55%)]\tLoss: 0.001578\nTrain Epoch: 9 [33280/60000 (55%)]\tLoss: 0.005359\nTrain Epoch: 9 [33600/60000 (56%)]\tLoss: 0.015033\nTrain Epoch: 9 [33920/60000 (57%)]\tLoss: 0.001892\nTrain Epoch: 9 [34240/60000 (57%)]\tLoss: 0.020740\nTrain Epoch: 9 [34560/60000 (58%)]\tLoss: 0.013471\nTrain Epoch: 9 [34880/60000 (58%)]\tLoss: 0.014709\nTrain Epoch: 9 [35200/60000 (59%)]\tLoss: 0.017029\nTrain Epoch: 9 [35520/60000 (59%)]\tLoss: 0.008606\nTrain Epoch: 9 [35840/60000 (60%)]\tLoss: 0.007703\nTrain Epoch: 9 [36160/60000 (60%)]\tLoss: 0.016644\nTrain Epoch: 9 [36480/60000 (61%)]\tLoss: 0.005909\nTrain Epoch: 9 [36800/60000 (61%)]\tLoss: 0.004779\nTrain Epoch: 9 [37120/60000 (62%)]\tLoss: 0.010394\nTrain Epoch: 9 [37440/60000 (62%)]\tLoss: 0.002865\nTrain Epoch: 9 [37760/60000 (63%)]\tLoss: 0.001243\nTrain Epoch: 9 [38080/60000 (63%)]\tLoss: 0.005614\nTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.164388\nTrain Epoch: 9 [38720/60000 (65%)]\tLoss: 0.002075\nTrain Epoch: 9 [39040/60000 (65%)]\tLoss: 0.001015\nTrain Epoch: 9 [39360/60000 (66%)]\tLoss: 0.001527\nTrain Epoch: 9 [39680/60000 (66%)]\tLoss: 0.004187\nTrain Epoch: 9 [40000/60000 (67%)]\tLoss: 0.001853\nTrain Epoch: 9 [40320/60000 (67%)]\tLoss: 0.005061\nTrain Epoch: 9 [40640/60000 (68%)]\tLoss: 0.001574\nTrain Epoch: 9 [40960/60000 (68%)]\tLoss: 0.003914\nTrain Epoch: 9 [41280/60000 (69%)]\tLoss: 0.015827\nTrain Epoch: 9 [41600/60000 (69%)]\tLoss: 0.001789\nTrain Epoch: 9 [41920/60000 (70%)]\tLoss: 0.007818\nTrain Epoch: 9 [42240/60000 (70%)]\tLoss: 0.004238\nTrain Epoch: 9 [42560/60000 (71%)]\tLoss: 0.003724\nTrain Epoch: 9 [42880/60000 (71%)]\tLoss: 0.018572\nTrain Epoch: 9 [43200/60000 (72%)]\tLoss: 0.000416\nTrain Epoch: 9 [43520/60000 (73%)]\tLoss: 0.012591\nTrain Epoch: 9 [43840/60000 (73%)]\tLoss: 0.002548\nTrain Epoch: 9 [44160/60000 (74%)]\tLoss: 0.029758\nTrain Epoch: 9 [44480/60000 (74%)]\tLoss: 0.000825\nTrain Epoch: 9 [44800/60000 (75%)]\tLoss: 0.017451\nTrain Epoch: 9 [45120/60000 (75%)]\tLoss: 0.003683\nTrain Epoch: 9 [45440/60000 (76%)]\tLoss: 0.003813\nTrain Epoch: 9 [45760/60000 (76%)]\tLoss: 0.025179\nTrain Epoch: 9 [46080/60000 (77%)]\tLoss: 0.004892\nTrain Epoch: 9 [46400/60000 (77%)]\tLoss: 0.024728\nTrain Epoch: 9 [46720/60000 (78%)]\tLoss: 0.028306\nTrain Epoch: 9 [47040/60000 (78%)]\tLoss: 0.007662\nTrain Epoch: 9 [47360/60000 (79%)]\tLoss: 0.012446\nTrain Epoch: 9 [47680/60000 (79%)]\tLoss: 0.042402\nTrain Epoch: 9 [48000/60000 (80%)]\tLoss: 0.003336\nTrain Epoch: 9 [48320/60000 (81%)]\tLoss: 0.001187\nTrain Epoch: 9 [48640/60000 (81%)]\tLoss: 0.002769\nTrain Epoch: 9 [48960/60000 (82%)]\tLoss: 0.001677\nTrain Epoch: 9 [49280/60000 (82%)]\tLoss: 0.025274\nTrain Epoch: 9 [49600/60000 (83%)]\tLoss: 0.007978\nTrain Epoch: 9 [49920/60000 (83%)]\tLoss: 0.085283\nTrain Epoch: 9 [50240/60000 (84%)]\tLoss: 0.074249\nTrain Epoch: 9 [50560/60000 (84%)]\tLoss: 0.094540\nTrain Epoch: 9 [50880/60000 (85%)]\tLoss: 0.009963\nTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.004449\nTrain Epoch: 9 [51520/60000 (86%)]\tLoss: 0.001475\nTrain Epoch: 9 [51840/60000 (86%)]\tLoss: 0.008012\nTrain Epoch: 9 [52160/60000 (87%)]\tLoss: 0.003974\nTrain Epoch: 9 [52480/60000 (87%)]\tLoss: 0.003187\nTrain Epoch: 9 [52800/60000 (88%)]\tLoss: 0.001051\nTrain Epoch: 9 [53120/60000 (89%)]\tLoss: 0.004100\nTrain Epoch: 9 [53440/60000 (89%)]\tLoss: 0.030190\nTrain Epoch: 9 [53760/60000 (90%)]\tLoss: 0.028403\nTrain Epoch: 9 [54080/60000 (90%)]\tLoss: 0.005450\nTrain Epoch: 9 [54400/60000 (91%)]\tLoss: 0.066674\nTrain Epoch: 9 [54720/60000 (91%)]\tLoss: 0.003263\nTrain Epoch: 9 [55040/60000 (92%)]\tLoss: 0.011885\nTrain Epoch: 9 [55360/60000 (92%)]\tLoss: 0.006557\nTrain Epoch: 9 [55680/60000 (93%)]\tLoss: 0.001908\nTrain Epoch: 9 [56000/60000 (93%)]\tLoss: 0.001726\nTrain Epoch: 9 [56320/60000 (94%)]\tLoss: 0.001058\nTrain Epoch: 9 [56640/60000 (94%)]\tLoss: 0.034565\nTrain Epoch: 9 [56960/60000 (95%)]\tLoss: 0.005606\nTrain Epoch: 9 [57280/60000 (95%)]\tLoss: 0.062359\nTrain Epoch: 9 [57600/60000 (96%)]\tLoss: 0.000306\nTrain Epoch: 9 [57920/60000 (97%)]\tLoss: 0.002289\nTrain Epoch: 9 [58240/60000 (97%)]\tLoss: 0.000454\nTrain Epoch: 9 [58560/60000 (98%)]\tLoss: 0.002211\nTrain Epoch: 9 [58880/60000 (98%)]\tLoss: 0.004284\nTrain Epoch: 9 [59200/60000 (99%)]\tLoss: 0.093780\nTrain Epoch: 9 [59520/60000 (99%)]\tLoss: 0.048144\nTrain Epoch: 9 [59840/60000 (100%)]\tLoss: 0.006778\n\nTest set: Average loss: 0.0000, Accuracy: 9786/10000 (98%)\n\nTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.002438\nTrain Epoch: 10 [320/60000 (1%)]\tLoss: 0.001927\nTrain Epoch: 10 [640/60000 (1%)]\tLoss: 0.002027\nTrain Epoch: 10 [960/60000 (2%)]\tLoss: 0.001046\nTrain Epoch: 10 [1280/60000 (2%)]\tLoss: 0.040134\nTrain Epoch: 10 [1600/60000 (3%)]\tLoss: 0.008338\nTrain Epoch: 10 [1920/60000 (3%)]\tLoss: 0.042010\nTrain Epoch: 10 [2240/60000 (4%)]\tLoss: 0.005646\nTrain Epoch: 10 [2560/60000 (4%)]\tLoss: 0.004570\nTrain Epoch: 10 [2880/60000 (5%)]\tLoss: 0.032071\nTrain Epoch: 10 [3200/60000 (5%)]\tLoss: 0.004234\nTrain Epoch: 10 [3520/60000 (6%)]\tLoss: 0.012212\nTrain Epoch: 10 [3840/60000 (6%)]\tLoss: 0.013116\nTrain Epoch: 10 [4160/60000 (7%)]\tLoss: 0.000285\nTrain Epoch: 10 [4480/60000 (7%)]\tLoss: 0.006279\nTrain Epoch: 10 [4800/60000 (8%)]\tLoss: 0.007270\nTrain Epoch: 10 [5120/60000 (9%)]\tLoss: 0.013955\nTrain Epoch: 10 [5440/60000 (9%)]\tLoss: 0.000773\nTrain Epoch: 10 [5760/60000 (10%)]\tLoss: 0.012950\nTrain Epoch: 10 [6080/60000 (10%)]\tLoss: 0.002984\nTrain Epoch: 10 [6400/60000 (11%)]\tLoss: 0.006609\nTrain Epoch: 10 [6720/60000 (11%)]\tLoss: 0.017686\nTrain Epoch: 10 [7040/60000 (12%)]\tLoss: 0.006369\nTrain Epoch: 10 [7360/60000 (12%)]\tLoss: 0.014334\nTrain Epoch: 10 [7680/60000 (13%)]\tLoss: 0.001595\nTrain Epoch: 10 [8000/60000 (13%)]\tLoss: 0.000196\nTrain Epoch: 10 [8320/60000 (14%)]\tLoss: 0.015443\nTrain Epoch: 10 [8640/60000 (14%)]\tLoss: 0.002724\nTrain Epoch: 10 [8960/60000 (15%)]\tLoss: 0.001893\nTrain Epoch: 10 [9280/60000 (15%)]\tLoss: 0.006514\nTrain Epoch: 10 [9600/60000 (16%)]\tLoss: 0.023164\nTrain Epoch: 10 [9920/60000 (17%)]\tLoss: 0.058266\nTrain Epoch: 10 [10240/60000 (17%)]\tLoss: 0.004144\nTrain Epoch: 10 [10560/60000 (18%)]\tLoss: 0.013443\nTrain Epoch: 10 [10880/60000 (18%)]\tLoss: 0.002687\nTrain Epoch: 10 [11200/60000 (19%)]\tLoss: 0.014791\nTrain Epoch: 10 [11520/60000 (19%)]\tLoss: 0.006688\nTrain Epoch: 10 [11840/60000 (20%)]\tLoss: 0.027535\nTrain Epoch: 10 [12160/60000 (20%)]\tLoss: 0.007300\nTrain Epoch: 10 [12480/60000 (21%)]\tLoss: 0.014730\nTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.002361\nTrain Epoch: 10 [13120/60000 (22%)]\tLoss: 0.011169\nTrain Epoch: 10 [13440/60000 (22%)]\tLoss: 0.036650\nTrain Epoch: 10 [13760/60000 (23%)]\tLoss: 0.000781\nTrain Epoch: 10 [14080/60000 (23%)]\tLoss: 0.000215\nTrain Epoch: 10 [14400/60000 (24%)]\tLoss: 0.003081\nTrain Epoch: 10 [14720/60000 (25%)]\tLoss: 0.006807\nTrain Epoch: 10 [15040/60000 (25%)]\tLoss: 0.005865\nTrain Epoch: 10 [15360/60000 (26%)]\tLoss: 0.003245\nTrain Epoch: 10 [15680/60000 (26%)]\tLoss: 0.002677\nTrain Epoch: 10 [16000/60000 (27%)]\tLoss: 0.003372\nTrain Epoch: 10 [16320/60000 (27%)]\tLoss: 0.002643\nTrain Epoch: 10 [16640/60000 (28%)]\tLoss: 0.009818\nTrain Epoch: 10 [16960/60000 (28%)]\tLoss: 0.015630\nTrain Epoch: 10 [17280/60000 (29%)]\tLoss: 0.004322\nTrain Epoch: 10 [17600/60000 (29%)]\tLoss: 0.029193\nTrain Epoch: 10 [17920/60000 (30%)]\tLoss: 0.009636\nTrain Epoch: 10 [18240/60000 (30%)]\tLoss: 0.012073\nTrain Epoch: 10 [18560/60000 (31%)]\tLoss: 0.009760\nTrain Epoch: 10 [18880/60000 (31%)]\tLoss: 0.029769\nTrain Epoch: 10 [19200/60000 (32%)]\tLoss: 0.001384\nTrain Epoch: 10 [19520/60000 (33%)]\tLoss: 0.063365\nTrain Epoch: 10 [19840/60000 (33%)]\tLoss: 0.001841\nTrain Epoch: 10 [20160/60000 (34%)]\tLoss: 0.004571\nTrain Epoch: 10 [20480/60000 (34%)]\tLoss: 0.004442\nTrain Epoch: 10 [20800/60000 (35%)]\tLoss: 0.005465\nTrain Epoch: 10 [21120/60000 (35%)]\tLoss: 0.045523\nTrain Epoch: 10 [21440/60000 (36%)]\tLoss: 0.016541\nTrain Epoch: 10 [21760/60000 (36%)]\tLoss: 0.001368\nTrain Epoch: 10 [22080/60000 (37%)]\tLoss: 0.003554\nTrain Epoch: 10 [22400/60000 (37%)]\tLoss: 0.015395\nTrain Epoch: 10 [22720/60000 (38%)]\tLoss: 0.003083\nTrain Epoch: 10 [23040/60000 (38%)]\tLoss: 0.005449\nTrain Epoch: 10 [23360/60000 (39%)]\tLoss: 0.001848\nTrain Epoch: 10 [23680/60000 (39%)]\tLoss: 0.005111\nTrain Epoch: 10 [24000/60000 (40%)]\tLoss: 0.013490\nTrain Epoch: 10 [24320/60000 (41%)]\tLoss: 0.054362\nTrain Epoch: 10 [24640/60000 (41%)]\tLoss: 0.003911\nTrain Epoch: 10 [24960/60000 (42%)]\tLoss: 0.025199\nTrain Epoch: 10 [25280/60000 (42%)]\tLoss: 0.003014\nTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.009158\nTrain Epoch: 10 [25920/60000 (43%)]\tLoss: 0.089876\nTrain Epoch: 10 [26240/60000 (44%)]\tLoss: 0.000954\nTrain Epoch: 10 [26560/60000 (44%)]\tLoss: 0.010848\nTrain Epoch: 10 [26880/60000 (45%)]\tLoss: 0.003463\nTrain Epoch: 10 [27200/60000 (45%)]\tLoss: 0.001384\nTrain Epoch: 10 [27520/60000 (46%)]\tLoss: 0.002877\nTrain Epoch: 10 [27840/60000 (46%)]\tLoss: 0.051419\nTrain Epoch: 10 [28160/60000 (47%)]\tLoss: 0.015037\nTrain Epoch: 10 [28480/60000 (47%)]\tLoss: 0.005866\nTrain Epoch: 10 [28800/60000 (48%)]\tLoss: 0.001524\nTrain Epoch: 10 [29120/60000 (49%)]\tLoss: 0.003802\nTrain Epoch: 10 [29440/60000 (49%)]\tLoss: 0.015547\nTrain Epoch: 10 [29760/60000 (50%)]\tLoss: 0.008850\nTrain Epoch: 10 [30080/60000 (50%)]\tLoss: 0.005989\nTrain Epoch: 10 [30400/60000 (51%)]\tLoss: 0.005111\nTrain Epoch: 10 [30720/60000 (51%)]\tLoss: 0.011522\nTrain Epoch: 10 [31040/60000 (52%)]\tLoss: 0.002915\nTrain Epoch: 10 [31360/60000 (52%)]\tLoss: 0.001119\nTrain Epoch: 10 [31680/60000 (53%)]\tLoss: 0.003960\nTrain Epoch: 10 [32000/60000 (53%)]\tLoss: 0.000588\nTrain Epoch: 10 [32320/60000 (54%)]\tLoss: 0.001462\nTrain Epoch: 10 [32640/60000 (54%)]\tLoss: 0.005018\nTrain Epoch: 10 [32960/60000 (55%)]\tLoss: 0.019250\nTrain Epoch: 10 [33280/60000 (55%)]\tLoss: 0.011539\nTrain Epoch: 10 [33600/60000 (56%)]\tLoss: 0.000167\nTrain Epoch: 10 [33920/60000 (57%)]\tLoss: 0.000957\nTrain Epoch: 10 [34240/60000 (57%)]\tLoss: 0.015767\nTrain Epoch: 10 [34560/60000 (58%)]\tLoss: 0.001263\nTrain Epoch: 10 [34880/60000 (58%)]\tLoss: 0.029760\nTrain Epoch: 10 [35200/60000 (59%)]\tLoss: 0.008099\nTrain Epoch: 10 [35520/60000 (59%)]\tLoss: 0.013436\nTrain Epoch: 10 [35840/60000 (60%)]\tLoss: 0.023010\nTrain Epoch: 10 [36160/60000 (60%)]\tLoss: 0.004794\nTrain Epoch: 10 [36480/60000 (61%)]\tLoss: 0.005272\nTrain Epoch: 10 [36800/60000 (61%)]\tLoss: 0.005939\nTrain Epoch: 10 [37120/60000 (62%)]\tLoss: 0.018840\nTrain Epoch: 10 [37440/60000 (62%)]\tLoss: 0.017733\nTrain Epoch: 10 [37760/60000 (63%)]\tLoss: 0.002026\nTrain Epoch: 10 [38080/60000 (63%)]\tLoss: 0.000636\nTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.016478\nTrain Epoch: 10 [38720/60000 (65%)]\tLoss: 0.004442\nTrain Epoch: 10 [39040/60000 (65%)]\tLoss: 0.000621\nTrain Epoch: 10 [39360/60000 (66%)]\tLoss: 0.012618\nTrain Epoch: 10 [39680/60000 (66%)]\tLoss: 0.004821\nTrain Epoch: 10 [40000/60000 (67%)]\tLoss: 0.021882\nTrain Epoch: 10 [40320/60000 (67%)]\tLoss: 0.017376\nTrain Epoch: 10 [40640/60000 (68%)]\tLoss: 0.010128\nTrain Epoch: 10 [40960/60000 (68%)]\tLoss: 0.012707\nTrain Epoch: 10 [41280/60000 (69%)]\tLoss: 0.006856\nTrain Epoch: 10 [41600/60000 (69%)]\tLoss: 0.006447\nTrain Epoch: 10 [41920/60000 (70%)]\tLoss: 0.016734\nTrain Epoch: 10 [42240/60000 (70%)]\tLoss: 0.011465\nTrain Epoch: 10 [42560/60000 (71%)]\tLoss: 0.000868\nTrain Epoch: 10 [42880/60000 (71%)]\tLoss: 0.000772\nTrain Epoch: 10 [43200/60000 (72%)]\tLoss: 0.002622\nTrain Epoch: 10 [43520/60000 (73%)]\tLoss: 0.007846\nTrain Epoch: 10 [43840/60000 (73%)]\tLoss: 0.003614\nTrain Epoch: 10 [44160/60000 (74%)]\tLoss: 0.028298\nTrain Epoch: 10 [44480/60000 (74%)]\tLoss: 0.000894\nTrain Epoch: 10 [44800/60000 (75%)]\tLoss: 0.020737\nTrain Epoch: 10 [45120/60000 (75%)]\tLoss: 0.002699\nTrain Epoch: 10 [45440/60000 (76%)]\tLoss: 0.000450\nTrain Epoch: 10 [45760/60000 (76%)]\tLoss: 0.001712\nTrain Epoch: 10 [46080/60000 (77%)]\tLoss: 0.004971\nTrain Epoch: 10 [46400/60000 (77%)]\tLoss: 0.001167\nTrain Epoch: 10 [46720/60000 (78%)]\tLoss: 0.000449\nTrain Epoch: 10 [47040/60000 (78%)]\tLoss: 0.068595\nTrain Epoch: 10 [47360/60000 (79%)]\tLoss: 0.017617\nTrain Epoch: 10 [47680/60000 (79%)]\tLoss: 0.025750\nTrain Epoch: 10 [48000/60000 (80%)]\tLoss: 0.007555\nTrain Epoch: 10 [48320/60000 (81%)]\tLoss: 0.005110\nTrain Epoch: 10 [48640/60000 (81%)]\tLoss: 0.001486\nTrain Epoch: 10 [48960/60000 (82%)]\tLoss: 0.001490\nTrain Epoch: 10 [49280/60000 (82%)]\tLoss: 0.001655\nTrain Epoch: 10 [49600/60000 (83%)]\tLoss: 0.035605\nTrain Epoch: 10 [49920/60000 (83%)]\tLoss: 0.009652\nTrain Epoch: 10 [50240/60000 (84%)]\tLoss: 0.006210\nTrain Epoch: 10 [50560/60000 (84%)]\tLoss: 0.001992\nTrain Epoch: 10 [50880/60000 (85%)]\tLoss: 0.128303\nTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.005742\nTrain Epoch: 10 [51520/60000 (86%)]\tLoss: 0.025893\nTrain Epoch: 10 [51840/60000 (86%)]\tLoss: 0.007205\nTrain Epoch: 10 [52160/60000 (87%)]\tLoss: 0.023499\nTrain Epoch: 10 [52480/60000 (87%)]\tLoss: 0.014888\nTrain Epoch: 10 [52800/60000 (88%)]\tLoss: 0.001536\nTrain Epoch: 10 [53120/60000 (89%)]\tLoss: 0.004358\nTrain Epoch: 10 [53440/60000 (89%)]\tLoss: 0.002275\nTrain Epoch: 10 [53760/60000 (90%)]\tLoss: 0.001611\nTrain Epoch: 10 [54080/60000 (90%)]\tLoss: 0.027349\nTrain Epoch: 10 [54400/60000 (91%)]\tLoss: 0.031662\nTrain Epoch: 10 [54720/60000 (91%)]\tLoss: 0.009443\nTrain Epoch: 10 [55040/60000 (92%)]\tLoss: 0.067102\nTrain Epoch: 10 [55360/60000 (92%)]\tLoss: 0.002945\nTrain Epoch: 10 [55680/60000 (93%)]\tLoss: 0.000661\nTrain Epoch: 10 [56000/60000 (93%)]\tLoss: 0.002289\nTrain Epoch: 10 [56320/60000 (94%)]\tLoss: 0.005140\nTrain Epoch: 10 [56640/60000 (94%)]\tLoss: 0.029680\nTrain Epoch: 10 [56960/60000 (95%)]\tLoss: 0.000301\nTrain Epoch: 10 [57280/60000 (95%)]\tLoss: 0.053889\nTrain Epoch: 10 [57600/60000 (96%)]\tLoss: 0.020785\nTrain Epoch: 10 [57920/60000 (97%)]\tLoss: 0.001603\nTrain Epoch: 10 [58240/60000 (97%)]\tLoss: 0.010264\nTrain Epoch: 10 [58560/60000 (98%)]\tLoss: 0.002566\nTrain Epoch: 10 [58880/60000 (98%)]\tLoss: 0.006725\nTrain Epoch: 10 [59200/60000 (99%)]\tLoss: 0.037508\nTrain Epoch: 10 [59520/60000 (99%)]\tLoss: 0.007391\nTrain Epoch: 10 [59840/60000 (100%)]\tLoss: 0.036444\n\nTest set: Average loss: 0.0000, Accuracy: 9803/10000 (98%)\n\n"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)\n",
    "\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch)\n",
    "    test(model, device, test_loader)\n",
    "\n",
    "torch.save(model.state_dict(), \"mnist_model.pt\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": [
     "outputPrepend"
    ]
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}